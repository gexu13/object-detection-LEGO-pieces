import os
import xml.etree.ElementTree as ET
import numpy as np
import cv2
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Conv2D, Flatten, Dense, Lambda
from tensorflow.keras.applications import MobileNetV2

# ========================== 1ï¸âƒ£ è®¾å®šæ•°æ®è·¯å¾„ ==========================
DATASET_DIR = "dataset_final"
TEST_DIR = os.path.join(DATASET_DIR, "test")  # æµ‹è¯•é›†è·¯å¾„

IMAGE_WIDTH = 224
IMAGE_HEIGHT = 224
MAX_BOXES = 10  # æœ€å¤šæ£€æµ‹ 10 ä¸ªç›®æ ‡

# ========================== 2ï¸âƒ£ è§£æ PASCAL VOC æ•°æ® ==========================
def parse_voc_annotation(xml_file):
    """è§£æ PASCAL VOC XML æ ‡æ³¨"""
    tree = ET.parse(xml_file)
    root = tree.getroot()
    boxes = []
    for obj in root.findall("object"):
        bbox = obj.find("bndbox")
        xmin = int(bbox.find("xmin").text) / IMAGE_WIDTH
        ymin = int(bbox.find("ymin").text) / IMAGE_HEIGHT
        xmax = int(bbox.find("xmax").text) / IMAGE_WIDTH
        ymax = int(bbox.find("ymax").text) / IMAGE_HEIGHT
        boxes.append([xmin, ymin, xmax, ymax])

    while len(boxes) < MAX_BOXES:
        boxes.append([0, 0, 0, 0])  # ç”¨ 0 å¡«å……
    return np.array(boxes[:MAX_BOXES], dtype=np.float32)

def load_image_and_annotation(image_path, annotation_path):
    """åŠ è½½å•å¼ å›¾ç‰‡åŠå…¶æ ‡æ³¨"""
    image = cv2.imread(image_path)
    image = cv2.resize(image, (IMAGE_WIDTH, IMAGE_HEIGHT))
    image = image.astype(np.float32) / 255.0  # å½’ä¸€åŒ–

    boxes = parse_voc_annotation(annotation_path)
    return image, boxes

def get_image_label_paths(image_dir, annotation_dir):
    """è·å–æ‰€æœ‰æµ‹è¯•å›¾ç‰‡å’Œæ ‡æ³¨è·¯å¾„"""
    image_paths, annotation_paths = [], []
    for filename in os.listdir(annotation_dir):
        xml_path = os.path.join(annotation_dir, filename)
        img_path = os.path.join(image_dir, filename.replace(".xml", ".jpg"))
        if os.path.exists(img_path):
            image_paths.append(img_path)
            annotation_paths.append(xml_path)
    return image_paths, annotation_paths

# è·å–æµ‹è¯•é›†è·¯å¾„
test_image_paths, test_annotation_paths = get_image_label_paths(
    os.path.join(TEST_DIR, "images"), os.path.join(TEST_DIR, "annotations")
)

# ========================== 3ï¸âƒ£ åŠ è½½å·²è®­ç»ƒæ¨¡å‹ ==========================
def build_ssd():
    base_model = MobileNetV2(input_shape=(224, 224, 3), include_top=False)
    x = base_model.output
    x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)
    x = Flatten()(x)
    x = Dense(10 * 4, activation='sigmoid')(x)  # 10 ä¸ª bounding boxï¼Œæ¯ä¸ª 4 ä¸ªåæ ‡
    x = Lambda(lambda y: tf.reshape(y, (-1, 10, 4)), output_shape=(10, 4))(x)  # âœ… æ˜ç¡®æŒ‡å®šè¾“å‡ºå½¢çŠ¶
    return Model(inputs=base_model.input, outputs=x)

# âœ… é‡æ–°æ„å»ºæ¨¡å‹
model = build_ssd()
print("âœ… æˆåŠŸé‡å»ºæ¨¡å‹ï¼")

# âœ… åªåŠ è½½è®­ç»ƒå¥½çš„æƒé‡ï¼Œè€Œä¸åŠ è½½æ¶æ„
model.load_weights("ssd_lego.h5")
print("âœ… æˆåŠŸåŠ è½½æƒé‡ï¼")

# ========================== 4ï¸âƒ£ è®¡ç®— mAP@0.5 ==========================
def iou(boxA, boxB):
    """è®¡ç®— IoU (Intersection over Union)"""
    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[2], boxB[2])
    yB = min(boxA[3], boxB[3])

    interArea = max(0, xB - xA) * max(0, yB - yA)
    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])
    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])

    denominator = float(boxAArea + boxBArea - interArea)

    # âœ… é¿å…é™¤ä»¥ 0
    if denominator == 0:
        return 0.0

    return interArea / denominator

def mean_average_precision(true_boxes, pred_boxes, iou_threshold=0.5):
    """è®¡ç®— mAP@0.5"""
    total_iou = 0
    num_samples = len(true_boxes)

    for true, pred in zip(true_boxes, pred_boxes):
        avg_iou = np.mean([iou(true_box, pred_box) for true_box, pred_box in zip(true, pred)])
        total_iou += avg_iou

    return total_iou / num_samples if num_samples > 0 else 0

def evaluate_model(model, image_paths, annotation_paths):
    """åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹å¹¶è®¡ç®— mAP@0.5"""
    all_true_boxes = []
    all_pred_boxes = []

    for img_path, ann_path in zip(image_paths, annotation_paths):
        image, true_boxes = load_image_and_annotation(img_path, ann_path)
        image = np.expand_dims(image, axis=0)  # å¢åŠ  batch ç»´åº¦
        pred_boxes = model.predict(image)[0]  # é¢„æµ‹çš„ bounding boxes

        all_true_boxes.append(true_boxes)
        all_pred_boxes.append(pred_boxes)
        print("ğŸ” çœŸå®æ¡†:", true_boxes)
        print("ğŸ” é¢„æµ‹æ¡†:", pred_boxes)

    # è®¡ç®— mAP@0.5
    map_score = mean_average_precision(all_true_boxes, all_pred_boxes, iou_threshold=0.5)
    print(f"ğŸ“Š æµ‹è¯•é›† mAP@0.5: {map_score:.4f}")


# if __name__ == "__main__":
#     # è¯„ä¼°æ¨¡å‹
#     print("ğŸ“ è¯„ä¼°æ¨¡å‹...")
#     evaluate_model(model, test_image_paths, test_annotation_paths)
#     print("âœ… è¯„ä¼°å®Œæˆï¼")




import matplotlib.pyplot as plt

def visualize_predictions(image_path, pred_boxes):
    """å¯è§†åŒ–æ¨¡å‹é¢„æµ‹çš„ bounding boxes"""
    image = cv2.imread(image_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # è½¬æ¢ä¸º RGB
    h, w, _ = image.shape  # è·å–åŸå§‹å›¾åƒå¤§å°

    plt.figure(figsize=(8, 8))
    plt.imshow(image)

    # ç”»å‡ºé¢„æµ‹æ¡†
    for box in pred_boxes:
        if sum(box) == 0:  # å¿½ç•¥å…¨ 0 çš„æ¡†
            continue
        xmin, ymin, xmax, ymax = box
        xmin, xmax = int(xmin * w), int(xmax * w)  # åå½’ä¸€åŒ–
        ymin, ymax = int(ymin * h), int(ymax * h)
        plt.gca().add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin,
                                          edgecolor='r', linewidth=2, fill=False))

    plt.title("Predicted Bounding Boxes")
    plt.show()

# ========================== è¿è¡Œå¯è§†åŒ– ==========================
# é€‰ä¸€å¼ æµ‹è¯•å›¾ç‰‡
sample_image_path = test_image_paths[0]  # å–æµ‹è¯•é›†ä¸­çš„ç¬¬ä¸€å¼ å›¾ç‰‡
sample_image = cv2.imread(sample_image_path)
sample_image = cv2.resize(sample_image, (IMAGE_WIDTH, IMAGE_HEIGHT))
sample_image = sample_image.astype(np.float32) / 255.0  # å½’ä¸€åŒ–
sample_image = np.expand_dims(sample_image, axis=0)  # å¢åŠ  batch ç»´åº¦

# è®©æ¨¡å‹é¢„æµ‹
sample_pred_boxes = model.predict(sample_image)[0]  # å–ç¬¬ä¸€ä¸ªæ ·æœ¬çš„é¢„æµ‹ç»“æœ
print("Predicted Bounding Boxes:", sample_pred_boxes)

# å¯è§†åŒ–é¢„æµ‹æ¡†
visualize_predictions(sample_image_path, sample_pred_boxes)