import os
import xml.etree.ElementTree as ET
import tensorflow as tf
import numpy as np
import cv2
import matplotlib.pyplot as plt
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Conv2D, Flatten, Dense, Lambda
from tensorflow.keras.losses import Huber
from tensorflow.keras.callbacks import ModelCheckpoint, Callback
from tqdm import tqdm

# ========================== è®¾å®šæ•°æ®è·¯å¾„ ==========================
DATASET_DIR = "dataset_final"
TRAIN_DIR = os.path.join(DATASET_DIR, "train")
VAL_DIR = os.path.join(DATASET_DIR, "val")

# è§£æ PASCAL VOC æ•°æ®
IMAGE_WIDTH = 224
IMAGE_HEIGHT = 224
MAX_BOXES = 10  # è®¾å®šæœ€å¤šæ£€æµ‹ 10 ä¸ªç›®æ ‡

def parse_voc_annotation(xml_file):
    """è§£æ PASCAL VOC æ ‡æ³¨"""
    tree = ET.parse(xml_file)
    root = tree.getroot()
    boxes = []
    for obj in root.findall("object"):
        bbox = obj.find("bndbox")
        xmin = int(bbox.find("xmin").text) / IMAGE_WIDTH
        ymin = int(bbox.find("ymin").text) / IMAGE_HEIGHT
        xmax = int(bbox.find("xmax").text) / IMAGE_WIDTH
        ymax = int(bbox.find("ymax").text) / IMAGE_HEIGHT
        boxes.append([xmin, ymin, xmax, ymax])

    while len(boxes) < MAX_BOXES:
        boxes.append([0, 0, 0, 0])  # ç”¨ 0 å¡«å……
    return np.array(boxes[:MAX_BOXES], dtype=np.float32)

def load_image_and_annotation(image_path, annotation_path):
    """åŠ è½½å•å¼ å›¾ç‰‡åŠå…¶æ ‡æ³¨"""
    if isinstance(image_path, tf.Tensor):
        image_path = image_path.numpy().decode("utf-8")
    if isinstance(annotation_path, tf.Tensor):
        annotation_path = annotation_path.numpy().decode("utf-8")

    print(f"loading image: {image_path}")
    image = cv2.imread(image_path)
    image = cv2.resize(image, (IMAGE_WIDTH, IMAGE_HEIGHT))  # SSD éœ€è¦ 224x224 è¾“å…¥
    image = image.astype(np.float32) / 255.0  # å½’ä¸€åŒ–

    boxes = parse_voc_annotation(annotation_path)
    return image, boxes

def get_image_label_paths(image_dir, annotation_dir):
    """è·å–æ‰€æœ‰å›¾ç‰‡å’Œæ ‡æ³¨è·¯å¾„"""
    image_paths, annotation_paths = [], []
    for filename in os.listdir(annotation_dir):
        xml_path = os.path.join(annotation_dir, filename)
        img_path = os.path.join(image_dir, filename.replace(".xml", ".jpg"))
        if os.path.exists(img_path):
            image_paths.append(img_path)
            annotation_paths.append(xml_path)
    return image_paths, annotation_paths

# è·å–æ•°æ®è·¯å¾„
train_image_paths, train_annotation_paths = get_image_label_paths(
    os.path.join(TRAIN_DIR, "images"), os.path.join(TRAIN_DIR, "annotations")
)
val_image_paths, val_annotation_paths = get_image_label_paths(
    os.path.join(VAL_DIR, "images"), os.path.join(VAL_DIR, "annotations")
)

# ========================== 3ï¸âƒ£ ç”Ÿæˆæ•°æ®é›† ==========================
def data_generator(image_paths, annotation_paths, batch_size=4):
    dataset = tf.data.Dataset.from_tensor_slices((image_paths, annotation_paths))

    def load_data(image_path, annotation_path):
        image, boxes = tf.py_function(load_image_and_annotation, [image_path, annotation_path], [tf.float32, tf.float32])
        image.set_shape((IMAGE_WIDTH, IMAGE_HEIGHT, 3))
        boxes.set_shape((MAX_BOXES, 4))  # å›ºå®š shape
        return image, boxes

    dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)
    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)
    return dataset

train_dataset = data_generator(train_image_paths, train_annotation_paths, batch_size=4)
val_dataset = data_generator(val_image_paths, val_annotation_paths, batch_size=4)

# ========================== 4ï¸âƒ£ SSD ç›®æ ‡æ£€æµ‹æ¨¡å‹ ==========================
def build_ssd():
    base_model = MobileNetV2(input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, 3), include_top=False)
    x = base_model.output
    x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)
    x = Flatten()(x)
    x = Dense(MAX_BOXES * 4, activation='sigmoid')(x)  # é¢„æµ‹ 10 ä¸ªæ¡†ï¼Œæ¯ä¸ªæ¡† 4 ä¸ªåæ ‡
    x = Lambda(lambda y: tf.reshape(y, (-1, MAX_BOXES, 4)))(x)
    return Model(inputs=base_model.input, outputs=x)

model = build_ssd()
model.compile(optimizer="adam", loss=Huber(delta=1.0))

# ========================== 5ï¸âƒ£ è¿›åº¦æ¡ & è®­ç»ƒ ==========================
class TQDMProgressBar(Callback):
    def on_train_begin(self, logs=None):
        self.epochs = self.params["epochs"]
        self.progress_bar = tqdm(total=self.epochs, desc="Training Progress")

    def on_epoch_end(self, epoch, logs=None):
        loss = logs.get("loss", 0)
        val_loss = logs.get("val_loss", 0)
        self.progress_bar.update(1)
        print(f"Epoch {epoch+1}/{self.epochs} - Loss: {loss:.4f} - Val Loss: {val_loss:.4f}")

    def on_train_end(self, logs=None):
        self.progress_bar.close()

# ========================== 6ï¸âƒ£ è®­ç»ƒ ==========================
EPOCHS = 20

# è®­ç»ƒæ¨¡å‹
print("ğŸš€ å¼€å§‹è®­ç»ƒ SSD æ¨¡å‹...")
history = model.fit(
    train_dataset,
    validation_data=val_dataset,
    epochs=EPOCHS,
    callbacks=[ModelCheckpoint("ssd_lego.h5", save_best_only=True), TQDMProgressBar()],
    verbose=2  # å…³é—­é»˜è®¤ Keras è¿›åº¦æ¡
)

print("âœ… è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜ä¸º ssd_lego.h5")

# ========================== 7ï¸âƒ£ ç»˜åˆ¶ Loss æ›²çº¿ ==========================
plt.figure()
plt.plot(history.history["loss"], label="Train Loss")
plt.plot(history.history["val_loss"], label="Val Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.title("Training Loss Curve")
plt.savefig("loss_curve.png")
print("ğŸ“‰ Loss æ›²çº¿å·²ä¿å­˜ä¸º loss_curve.png")